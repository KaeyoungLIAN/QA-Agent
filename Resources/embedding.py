# -*- coding: utf-8 -*-
"""
ä»ä¸¤ä¸ª JSONLï¼ˆutterances / flows çš„åˆ†å—ç»“æœï¼‰æ„å»º bge-m3 + FAISS å‘é‡åº“ã€‚
- ä¼˜å…ˆä»æœ¬åœ°è·¯å¾„åŠ è½½ bge-m3 æ¨¡å‹ï¼ˆå‡å°‘é‡å¤ä¸‹è½½ï¼‰
- è¯»å– JSONLï¼ˆæ¯è¡Œå¿…é¡»åŒ…å«: text, metadataï¼‰
- ç”¨ bge-m3 è®¡ç®—å‘é‡ï¼Œå¹¶åš L2 å½’ä¸€åŒ–
- å»ºç«‹ IndexFlatIPï¼ˆå†…ç§¯ï¼‰ç´¢å¼•å¹¶è½ç›˜
- å†™å‡º meta.jsonlï¼ˆä¸ç´¢å¼•å‘é‡é¡ºåºä¸¥æ ¼å¯¹é½ï¼‰
- æä¾›ä¸€ä¸ªæ£€ç´¢ demo

ä¾èµ–:
pip install sentence-transformers faiss-cpu tqdm ujson huggingface_hub
"""

import os
from pathlib import Path
from typing import List, Dict, Any
import ujson as json
from tqdm import tqdm
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# =========================
# é…ç½®
# =========================
JSONL_FILES = [
    "Resources/utterances/utterances_chunks.jsonl",
    "Resources/flows/flows_chunks.jsonl",
]
OUT_DIR = "faiss_store"
LOCAL_MODEL_DIR = r"D:\huggingface/models/bge-m3"  # æœ¬åœ° bge-m3 æ¨¡å‹ç›®å½•
HF_MODEL_ID = "BAAI/bge-m3"  # å¦‚æœæœ¬åœ°ä¸å­˜åœ¨åˆ™ä» Hugging Face ä¸‹è½½
BATCH_SIZE = 64
TOP_K = 5
TEXT_FIELD = "text"
META_FIELD = "metadata"


# =========================
# å·¥å…·å‡½æ•°
# =========================
def get_model_path(local_dir: str, hf_id: str) -> str:
    """ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½ã€‚"""
    from huggingface_hub import snapshot_download
    if os.path.exists(local_dir) and any(
        f in os.listdir(local_dir) for f in ["pytorch_model.bin", "model.safetensors"]
    ):
        print(f"ğŸ“‚ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_dir}")
        return local_dir
    else:
        exit()


def load_jsonl(path: str, namespace: str) -> List[Dict[str, Any]]:
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            rec = json.loads(line)
            if TEXT_FIELD not in rec:
                raise KeyError(f"{path} ç¼ºå°‘ '{TEXT_FIELD}' å­—æ®µ")
            if META_FIELD not in rec or not isinstance(rec[META_FIELD], dict):
                rec[META_FIELD] = {}
            rec[META_FIELD]["namespace"] = namespace
            data.append(rec)
    return data


def embed_chunks(chunks: List[Dict[str, Any]], model_path: str, batch_size: int) -> np.ndarray:
    model = SentenceTransformer(model_path)
    texts = [c[TEXT_FIELD] for c in chunks]
    vecs = model.encode(texts, batch_size=batch_size, normalize_embeddings=True, show_progress_bar=True)
    return np.asarray(vecs, dtype="float32")


def build_faiss(embeddings: np.ndarray) -> faiss.Index:
    d = embeddings.shape[1]
    index = faiss.IndexFlatIP(d)
    index.add(embeddings)
    return index


def save_sidecar_meta(chunks: List[Dict[str, Any]], out_path: str):
    with open(out_path, "w", encoding="utf-8") as f:
        for c in chunks:
            out = {"id": c.get("id"), "text": c.get(TEXT_FIELD, ""), "metadata": c.get(META_FIELD, {})}
            f.write(json.dumps(out, ensure_ascii=False) + "\n")


def load_sidecar_meta(path: str) -> List[Dict[str, Any]]:
    return [json.loads(line) for line in open(path, "r", encoding="utf-8")]


def demo_search(query: str, index: faiss.Index, meta: List[Dict[str, Any]], model_path: str, top_k: int):
    model = SentenceTransformer(model_path)
    q = model.encode([query], normalize_embeddings=True)
    scores, ids = index.search(np.asarray(q, dtype="float32"), top_k)
    print(f"\nğŸ” Query: {query}")
    for rank, (i, s) in enumerate(zip(ids[0], scores[0]), start=1):
        if i == -1:
            continue
        item = meta[i]
        ns = item["metadata"].get("namespace")
        sec = item["metadata"].get("section_path", "")
        src = item["metadata"].get("source", "")
        preview = item["text"][:200].replace("\n", " ")
        print(f"{rank:>2}. [score={s:.4f}] [{ns}] {src} | {sec}")
        print(f"    {preview}{'...' if len(item['text'])>200 else ''}")


# =========================
# ä¸»æµç¨‹
# =========================
def main():
    os.makedirs(OUT_DIR, exist_ok=True)

    all_chunks = []
    for p in JSONL_FILES:
        path = Path(p)
        if not path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{path.resolve()}")
        ns = path.stem
        all_chunks.extend(load_jsonl(str(path), namespace=ns))

    if not all_chunks:
        raise RuntimeError("æœªä» JSONL è¯»å–åˆ°ä»»ä½•åˆ†å—ã€‚")

    print(f"âœ… åŠ è½½å®Œæˆï¼š{len(all_chunks)} ä¸ªåˆ†å—")

    model_path = get_model_path(LOCAL_MODEL_DIR, HF_MODEL_ID)

    embeddings = embed_chunks(all_chunks, model_path, BATCH_SIZE)
    print(f"âœ… åµŒå…¥å®Œæˆï¼šshape={embeddings.shape}")

    index = build_faiss(embeddings)
    faiss_path = os.path.join(OUT_DIR, "index.faiss")
    faiss.write_index(index, faiss_path)
    print(f"âœ… FAISS å·²ä¿å­˜ï¼š{faiss_path}")

    meta_path = os.path.join(OUT_DIR, "meta.jsonl")
    save_sidecar_meta(all_chunks, meta_path)
    print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜ï¼š{meta_path}")

    print("\nâ€”â€” æ£€ç´¢ Demo â€”â€”")
    index2 = faiss.read_index(faiss_path)
    meta2 = load_sidecar_meta(meta_path)
    demo_search("æŸ¥è¯¢è®¢å•çŠ¶æ€ æ€ä¹ˆè¯´ï¼Ÿ", index2, meta2, model_path, TOP_K)
    demo_search("ç”³è¯·é€€è´§çš„æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ", index2, meta2, model_path, TOP_K)


if __name__ == "__main__":
    main()
